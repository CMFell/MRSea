\documentclass[10pt, a4paper]{article}
\usepackage{fancyhdr}
\usepackage{bookman}
\usepackage{amsthm}
\usepackage{natbib}

\pagestyle{fancy} \rhead[RH-even]{}
\setlength{\oddsidemargin}{0pt} \setlength{\evensidemargin}{0pt}
\setlength{\textwidth}{450pt}
\newcommand{\markforthis}[1]{\nolinebreak\hfill [#1]}


\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
# set global chunk options
require(knitr)
opts_chunk$set(fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=60)
@
\begin{titlepage}


\begin{center}
\vspace* {0.70 in}

\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}

\huge{Vignette for the {\tt MRSea} Package (v0.2.0)}\\[0.5 cm]
%\Large{University of St. Andrews}\\
\Large{Statistical Modelling of bird and cetacean distributions in offshore renewables development areas}\\
\vspace{0.3 in}
\today

\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}


\vspace{1 in}
\Large{Lindesay Scott-Hayward}\\
\Large{Cornelia Oedekoven}\\
\Large{Monique Mackenzie}\\
\Large{Cameron Walker}\\[0.5 cm]


\vspace{1.6 in}
\begin{center}
\large{This vignette constitutes work carried out at the Centre for Research into Ecological and Environmental Modelling (CREEM) at the University of St. Andrews.}
\end{center}


\thispagestyle{empty}

\end{center}

\end{titlepage}

\vspace{0.3cm}
\textbf{Please reference this document as:}
\noindent Scott-Hayward, L.A.S., Oedekoven, C.S., Mackenzie, M.L. and Walker, C.G. (2015). Vignette for the MRSea Package v0.2.0: Statistical Modelling of bird and cetacean distributions in offshore renewables development areas. Centre for Research into Ecological and Environmental Modelling, University of St Andrews.

<<message=FALSE, echo=FALSE, warning=FALSE>>=
require(knitcitations)
cleanbib()
#require(bibtex)
# write.bibtex(c('mrds', 'geepack', 'fields', 'car'), file='packref')
# bibs<-read.bibtex('packref')
# bibs2<-read.bibtex('refs.bib')
# write.bibtex(c(bibs, bibs2), 'newref')
biblio <- read.bibtex("newref.bib")
cite_options(citation_format = 'compatability', cite.style = 'authoryear', max.names = 1, longnamesfirst=FALSE)
#biblio[[1]]
@

%


%~~~~~~~~~~~~~~~~~~~~~~~~
\section{Introduction}
The {\tt MRSea} package was developed for analysing data that was collected for assessing potential impacts of renewable developments on marine wildlife, although the methods are applicable to other studies as well. This vignette gives an updated example of the code for version 0.2.0.  For additional information regarding methods, see \Sexpr{citet(biblio[['Mac2013']])} and \Sexpr{citet(biblio[['ScottH2013a']])}.  The user should be familiar with generalised linear models and their assumptions and model selection. The {\tt MRSea} package primarily allows spatially adaptive model selection for both one and two dimensional covariates using the functions {\tt runSALSA1D\_withremoval} and {\tt runSALSA2D}, which implement the methods of \Sexpr{citet(biblio[['Walker2010']])} and \Sexpr{citet(biblio[['ScottH2013']])}. Other functions include diagnostics (to assess residual correlation: {\tt runACF}, smooth relationships: {\tt runPartialPlots} and model selection (ANOVA) for a Generalised Estimating Equation used when residual correlation is present: {\tt getPvalues}) and inference ({\tt do.bootstrap.cress}). 


\begin{figure}[!h]
{\centering \includegraphics[width=0.8\maxwidth]{MRSea_workflow.png} 
\caption{Example of the modelling process using MRSea.  Packages with functions to run certain parts are given in oval boxes.  To complete the modelling process, other packages may be used at certain stages \Sexpr{citep(biblio[[c('geepack1', 'mrds', 'car')]])}.  These are coded light blue, whilst MRSea functions are in red.  GEE (far right), stands for Generalised Estimating Equations.}
}
\end{figure}

\vspace{0.3cm}
A full description of each of the functions within the {\tt MRSea} package can be found in the reference manual at: \\
\href{http://creem2.st-and.ac.uk/software.aspx}{http://creem2.st-and.ac.uk/software.aspx}.  The manual and this document use version 0.2.0 of MRSea.

\vspace{0.3cm}

\begin{enumerate}

\section{Distance sampling using the {\tt mrds} library}

\item Load data and fit detection function (Distance Sampling)
<<message=FALSE>>=
require(MRSea)
# we will use the dataset with a known re-distribution of animals
data(dis.data.re)
dis.data<-dis.data.re
require(mrds) # distance sampling package
result <- ddf(dsmodel=~mcds(key="hn", formula=~1),
              data = dis.data, method="ds", 
              meta.data=list(width=250))
@

\item Adjust sightings for detectability 
<<dist, cache=TRUE, results='hide'>>=
# create.NHAT and create.count.data are MRSea functions to adjust the 
# sightings for the detection function estimated above.
dis.data <- create.NHAT(dis.data,result)
count.data <- create.count.data(dis.data)
@

\item Try a simple model
<<>>=
data <- count.data
data$response <- round(data$NHAT)
attach(data)
fullModel <- glm(response ~ as.factor(season) + as.factor(impact) +
                   depth + x.pos + y.pos, family = poisson, data = data)
@

\item Try a model with a smooth term for depth
<<message=FALSE>>=
#knots <- mean(depth) # must be specified as an object
require(splines)
fullModel <- glm(response ~ as.factor(season) + as.factor(impact) +
                   bs(depth, knots = mean(depth)) + x.pos + y.pos, 
                 family = poisson,data = data)
@

\item SALSA1D requires that {\tt foldid} is a column in the data set so that $k$-fold Cross-Validation (CV) may be used.  The user may specify how many folds (5 or 10 is usual) and whether or not the data has a blocking structure.  If the data are correlated then when selecting folds for the CV, the blocks must not be split up.


For correlated data:
<<>>=
# for correlated data:
data$blockid <- paste(data$transect.id, data$season, data$impact,sep = "")
data$foldid <- getCVids(data = data, folds = 5, block = 'blockid')
@

or uncorrelated data:
<<eval=FALSE>>=
# for uncorrelated data@
data$foldid<- getCVids(data=data, folds=5)
@

\section{Selection of 1D Covariates}
Run SALSA1D to select what covariates are included and whether or not they are smooth.  SALSA selects the smoothness of each term (number and location of knots) and CV is used to choose between the best smooth term, a linear term or no term at all.  To not allow the removal process the user may set {\tt removal = FALSE} as a parameter in the function {\tt runSALSA1D\_withremoval}.

\item Specify the parameters required:

<<>>=
salsa1dlist <- list(fitnessMeasure = "AICh", minKnots_1d = 2,maxKnots_1d = 5, 
                    startKnots_1d = 1, degree = 2, maxIterations = 10,
                    gaps = c(1))
@

\item If you wish to make predictions once the model is fitted, then a prediction grid should be created and specified.  This is because the splines fitted here (B-splines) are unable to make predictions outside of the range they were created.  For example, if the data range for depth is smaller than the range of depths in the prediction data, predictions cannot be made.  
<<>>=
data(predict.data.re)  # contains predict.data
# This is a spatial grid for making predictions.  All covariates in 
# final model must be in this data frame and the naming must be the 
# same as for the data
predictData <- predict.data.re
range(data$depth)
range(predictData$depth)
@

Here the range of the predictions is slightly wider than the range of the data, so we will specify {\tt predictData} when running SALSA.

\item Set up the initial model with factor covariates and the offset term (if required), and run SALSA.
<<message=FALSE>>=
initialModel <- glm(response ~ as.factor(season) + as.factor(impact) 
                    + offset(log(area)), family = "quasipoisson", 
                    data = data)
@

<<message=FALSE, warning=FALSE, echo=FALSE, results='hide'>>=
# run SALSA
salsa1dOutput <- runSALSA1D_withremoval(initialModel, salsa1dlist, c("depth"),
                      predictionData=predictData, datain=data, removal=TRUE)
@

<<eval=FALSE>>=
# run SALSA
salsa1dOutput <- runSALSA1D_withremoval(initialModel, salsa1dlist, c("depth"),
                      predictionData=predictData, datain=data, removal=TRUE)
@

<<>>=
# How many knots were chosen for depth?
salsa1dOutput$splineParams[[2]]$knots
splineParams<-salsa1dOutput$splineParams
# ~~~~~~~~~~~~~~~~~~~~~~~
@


\section{Selection of flexibility for 2D smooth term}

\item Create a grid of knots that will be used as possible knot locations.  This may take while and could be different every time you run it so I suggest saving the knotgrid as a file.

<<knotgrid, message=FALSE,cache=TRUE, fig=TRUE, fig.align='center', fig.width=9, fig.height=6, out.width='0.9\\linewidth', fig.pos='!h'>>=
knotgrid<- getKnotgrid(coordData = cbind(data$x.pos, data$y.pos))
#
# write.csv(knotgrid, file='knotgrid_fullanalysis.csv', row.names=F)
# ~~~~~~~~~~~~~~~~~~~~~~~
@
The black points in the figure are the data and the red points, the candidate knot locations.

\item Set up parameters for SALSA2D.  Distance matrices and the range parameter.  The default setting of 10 radii should be fine for general use.  Choose a fit statistic, min, max and start knots.

<<>>=
# make distance matrices for datatoknots and knottoknots
distMats <- makeDists(cbind(data$x.pos, data$y.pos), na.omit(knotgrid))

r_seq <- getRadiiChoices(numberofradii=10, distMatrix=distMats$dataDist)

# ~~~~~~~~~~~~~~~~~~~~~~~

# make parameter set for running salsa2d
salsa2dlist<-list(fitnessMeasure = 'QICb', knotgrid = knotgrid, 
                  knotdim=c(100,100), startKnots=10, minKnots=4,
                  maxKnots=12, r_seq=r_seq, gap=0, 
                  interactionTerm="as.factor(impact)")
@

\item Run SALSA2D to find the appropriate number and location of knots for the 2D smooth term of {\tt x.pos} and {\tt y.pos}.
<<echo=FALSE, message=FALSE, warning=FALSE, results='hide'>>=
salsa2dOutput_k6<-runSALSA2D(salsa1dOutput$bestModel, salsa2dlist, 
                      d2k=distMats$dataDist,k2k=distMats$knotDist, 
                      splineParams=splineParams)
@

<<echo=TRUE, eval=FALSE>>=
salsa2dOutput_k6<-runSALSA2D(salsa1dOutput$bestModel, salsa2dlist, 
                             d2k=distMats$dataDist, k2k=distMats$knotDist, 
                             splineParams=splineParams)
@
\item Update relevent SALSA parameters in your workspace (this is important for updating the model or making predictions).

<<>>=
splineParams<-salsa2dOutput_k6$splineParams
# specify parameters for local radial function:
radiusIndices <- splineParams[[1]]$radiusIndices
dists <- splineParams[[1]]$dist
radii <- splineParams[[1]]$radii
aR <- splineParams[[1]]$invInd[splineParams[[1]]$knotPos]
@

\item Are the residuals correlated? Make a suitable blocking structure, within which residuals are expected to be correlated but between which they are independent.  Use {\tt runACF} to assess the blocking structure.

<<eval=FALSE>>=
data$blockid<-paste(data$transect.id, data$season, data$impact, sep='')
runACF(block = data$blockid, model = salsa2dOutput_k6$bestModel)
@

<<eval=TRUE, echo=FALSE>>=
data$blockid<-paste(data$transect.id, data$season, data$impact, sep='')
@

\begin{figure}
{\centering \includegraphics[width=0.6\maxwidth]{figure/unnamed-chunk-13} 
\caption{ACF plot showing correlation in each block (grey lines), and the mean correlation by lag across blocks (red line).}
}
\end{figure}


Update the model to run a GEE to account for the correlation in the residuals (seen in the ACF plot).  The point estimates do not change as an independent working correlation structure is used, however, the standard errors are now appropriate and may be used for inference.

<<message=FALSE>>=
# Re-fit the chosen model as a GEE (based on SALSA knot placement) and 
# GEE p-values
require(geepack)
geeModel<- geeglm(formula(salsa2dOutput_k6$bestModel), data=data, 
                  family=poisson, id=blockid)
@

\item Check for model selection
<<>>=
getPvalues(model = geeModel, varlist = 'depth', 
           factorlist = c('season', 'impact'))
@

<<fig=TRUE, fig.align='center', fig.width=9, fig.height=6, out.width='0.5\\linewidth', fig.pos='!h', message=FALSE>>=
par(mfrow=c(2,2))
runPartialPlots(model = geeModel, data = data, factorlist = 
                  c('season', 'impact'), varlist = 'depth', showKnots = T)
@

\section{Making Predictions}

<<>>=
dists<-makeDists(cbind(predictData$x.pos, predictData$y.pos), 
                 na.omit(knotgrid),knotmat=FALSE)$dataDist

# make predictions on response scale
preds<-predict.cress(predictData, splineParams, dists, geeModel)
@

Plotting the predictions pre and post impact:
<<fig=TRUE, fig.align='center', fig.width=9, fig.height=6, out.width='0.9\\linewidth', fig.pos='!h'>>=
par(mfrow=c(1,2))
quilt.plot(predictData$x.pos[predictData$impact==0], 
           predictData$y.pos[predictData$impact==0], 
           preds[predictData$impact==0], nrow=104, ncol=55, asp=1)

quilt.plot(predictData$x.pos[predictData$impact==1], 
           predictData$y.pos[predictData$impact==1], 
           preds[predictData$impact==1], nrow=104, ncol=55, asp=1)
@

\section{Bootstrapped Confidence Intervals and Difference Surfaces}

\item The coding in this section has not changed from the original user guide.
\item Bootstrap to include parameter estimation uncertainty in the detection function and parameter estimation in the spatial model. (Note: If no detection function estimated, then the bootstrap is just on the parameters of the spatial model.)

<<boots, eval=FALSE>>=
dis.data$seasonimpact <- paste(dis.data$season, dis.data$impact)
do.bootstrap.cress(dis.data, predict.data=predictData, result, geeModel, 
                   splineParams, dists, resample = "transect.id",
                   rename = "segment.id", stratum = "seasonimpact", B = 100)
@

<<>>=
load('predictionboot.RData')
cis <- makeBootCIs(bootPreds)
@

\item Calculate the differences before and after across all bootstraps
<<>>=
differences <- getDifferences(beforePreds = 
                      bootPreds[predictData$impact == 0, ],
                      afterPreds = bootPreds[predictData$impact == 1, ])
@

\item Plot differences and indicate where significant positive/negative differences lie.
<<fig=TRUE, fig.align='center', fig.width=9, fig.height=6, out.width='0.9\\linewidth', fig.pos='!h'>>=
mediandiff <- differences$mediandiff
# The marker for each after - before difference:
# positive ('1') and negative ('-') significant differences
marker <- differences$significanceMarker
par(mfrow = c(1, 1))
quilt.plot(predictData$x.pos[predictData$impact == 0], 
           predictData$y.pos[predictData$impact == 0],
           mediandiff, asp = 1, nrow = 104, ncol = 55)
# add + or - depending on significance of cells. Just
# requires one significance out of all to be allocated
points(predictData$x.pos[predictData$impact == 0][marker == 1],
       predictData$y.pos[predictData$impact == 0][marker == 1],
       pch = "+", col = "darkgrey", cex = 0.75)
points(predictData$x.pos[predictData$impact == 0][marker == (-1)],
       predictData$y.pos[predictData$impact == 0][marker == (-1)],
       col = "darkgrey", cex = 0.75)
points(681417.3, 6046910, cex = 3, pch = "*", lwd = 1, col = "grey")
@


\end{enumerate}

\newpage
\section{References:}
% <<comment='', results='asis', echo=FALSE, message=FALSE>>=
% bibliography(sorting='ny', style = 'ecology', max.names=10)
% @
% %\bibliographystyle{}
% %\bibliography{newref}


\end{document}